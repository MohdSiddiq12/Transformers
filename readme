# Transformer Architecture

![Transformer Pic]<img src="https://user-images.githubusercontent.com/.../b8eb2fc1-f898-4810-b404-1afc270a9ece" alt="Transformer Pic">

The Transformer model, based on the Encoder-Decoder structure, has revolutionized natural language processing tasks. This README provides an overview of its architecture and components.

## Table of Contents
1. [Input Processing](#input-processing)
2. [Encoder](#encoder)
3. [Decoder](#decoder)
4. [Output Generation](#output-generation)

## Input Processing

### Input Embeddings
- Convert input tokens into fixed-size vectors
- Map each word in the vocabulary to a high-dimensional space

### Positional Encoding
- Add information about word position in the sequence
- Use sine and cosine functions of different frequencies

## Encoder

The Encoder consists of multiple identical layers, each containing:

### 1. Multi-Head Self-Attention Mechanism
- **Scaled Dot-Product Attention**
  - Compute attention scores using queries (Q), keys (K), and values (V)
  - Scale scores and apply softmax to obtain weights
  - Output weighted sum of values
- **Multi-Head Attention**
  - Use multiple attention heads to capture different aspects of input

### 2. Feed Forward Neural Network
- Apply two linear transformations with ReLU activation

> Note: Each sub-layer has a residual connection and layer normalization

## Decoder

The Decoder also consists of multiple identical layers, each containing:

### 1. Masked Multi-Head Self-Attention Mechanism
- Similar to encoder's self-attention, but with masking to prevent attending to future positions

### 2. Multi-Head Attention Mechanism (encoder-decoder attention)
- Perform attention over encoder's output and decoder's previous layer output

### 3. Feed Forward Neural Network
- Identical to the encoder's feed-forward network

> Note: Each sub-layer has a residual connection and layer normalization

## Output Generation

### Final Linear Layer
- Project decoder output into vocabulary space

### Softmax Layer
- Convert linear output to probabilities for each word in vocabulary

## Data Flow

1. Tokenize and embed input sequence, add positional encoding
2. Pass through multiple encoder layers
3. Process output sequence through decoder layers
4. Generate final output through linear transformation and softmax

The Transformer's power lies in its ability to capture long-range dependencies and parallelize computations, making it highly efficient for large-scale training.
