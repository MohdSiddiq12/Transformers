# ğŸš€ Transformer Architecture

<p align="center">
  <img src="https://github.com/user-attachments/assets/2fd80af4-1502-4c98-8242-c21c09195068" alt="Transformer Model Architecture" width="600">
</p>

<p align="center">
  <em>Revolutionizing Natural Language Processing with the Transformer Model</em>
</p>

## ğŸ“š Table of Contents

- [ğŸ¨ Input Processing](#-input-processing)
- [ğŸ—ï¸ Encoder](#%EF%B8%8F-encoder)
- [ğŸ§  Decoder](#-decoder)
- [ğŸ‰ Output Generation](#-output-generation)
- [ğŸ”„ Data Flow](#-data-flow)

## ğŸ¨ Input Processing

### ğŸ“Š Input Embeddings
- Transform input tokens into fixed-size vectors
- Map vocabulary words to high-dimensional space

### ğŸ”¢ Positional Encoding
- Inject sequence order information
- Utilize sine and cosine functions of varying frequencies

## ğŸ—ï¸ Encoder

The Encoder is a stack of identical layers, each featuring:

### 1. ğŸ‘ï¸ Multi-Head Self-Attention Mechanism

#### ğŸ” Scaled Dot-Product Attention
- Compute attention scores with queries (Q), keys (K), and values (V)
- Scale scores and apply softmax for weights
- Output weighted sum of values

#### ğŸ”€ Multi-Head Attention
- Employ multiple attention heads to capture diverse input aspects

### 2. ğŸ§® Feed Forward Neural Network
- Apply dual linear transformations with ReLU activation

> ğŸ’¡ Each sub-layer includes a residual connection and layer normalization

## ğŸ§  Decoder

The Decoder mirrors the Encoder's structure with additional components:

### 1. ğŸ­ Masked Multi-Head Self-Attention
- Similar to encoder's self-attention, but masks future positions

### 2. ğŸ”— Multi-Head Attention (encoder-decoder attention)
- Attend to encoder's output and previous decoder layer

### 3. ğŸ§® Feed Forward Neural Network
- Identical to the encoder's feed-forward network

> ğŸ’¡ Each sub-layer includes a residual connection and layer normalization

## ğŸ‰ Output Generation

### ğŸ“ Final Linear Layer
- Project decoder output onto vocabulary space

### ğŸŒŸ Softmax Layer
- Convert linear output to word probabilities

## ğŸ”„ Data Flow

1. ğŸ”¤ Tokenize, embed, and add positional encoding to input
2. ğŸ—ï¸ Process through multiple encoder layers
3. ğŸ§  Pass output sequence through decoder layers
4. ğŸ‰ Generate final output via linear transformation and softmax

---

<p align="center">
  <strong>The Transformer's Superpower</strong>: Capturing long-range dependencies and enabling parallel computations for efficient large-scale training.
</p>
